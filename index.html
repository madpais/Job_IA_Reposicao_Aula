<!DOCTYPE html>
<html lang="pt-br">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="style.css">
  <link rel="shortcut icon" href="img/favicon.jpg" type="image/x-icon">
  <title>Evolução da Intelgigência Artificial</title>
</head>

<body>

  <header>
    <h1>INTELIGÊNCIA ARTIFICIAL</h1>

  </header>


  <div class="timeline">
    
    <div class="container esquerda">
      <div class="conteudo">
        <div class="flex">
          <h2>1837</h2>
        <img src="img/1837.jpeg" width="200px">
        </div>
        <p>Charles Babbage projetou as bases para o desenvolvimento dos computadores como os conhecemos hoje. Seu principal projeto, a máquina diferencial, sendo precursora dos cumputadores modernos, mas nunca conseguiu concretizar o projeto.</p>
      </div>
    </div>
    
    <div class="container direita">
      <div class="conteudo">
        <div class="flex">
          <h2>1843</h2>
          <img src="img/1843.png" width="200px">
        </div>
        <p>Em meados do século XIX, a matemática e escritora britânica Ada Lovelace ousou ir além das limitações de sua
          época e criou o primeiro algoritmo destinado a ser processado por uma máquina. Essa contribuição mudou o rumo
          de tudo que foi criado nas décadas seguintes.</p>
        </div>
      </div>
      
      <div class="container esquerda">
      <div class="conteudo">
        <div class="flex">
          <h2>1888</h2>
          <img src="img/1888.jpeg" width="200px">
        </div>
        <p>Santiago Ramón y Cajal foi o primeiro a criar a <q>doutrina</q> do neurônio. Médico patologista espanhol
          descreveu a anatomia celular do cérebro e demonstrou mudanças dos neurônios durante o funcionamento do sistema
          nervoso , por este feito, hoje é conhecido como o pai da neurociência moderna. </p>
        </div>
      </div>
      
      <div class="container direita">
        <div class="conteudo">
          <div class="flex">
            <h2>1943</h2>
            <img src="img/1943.png" width="200px">
          </div>
        <p>A origem da IA data de 1943, quando Warren McCulloch e Walter Pitts criaram o primeiro modelo computacional
          para redes neurais.</p>
        </div>
      </div>
      
      <div class="container esquerda">
        <div class="conteudo">
          <div class="flex">
            <h2>1950</h2>
            <img src="img/1950.webp" width="200px">
        </div>
        <p>Quando os primeiros computadores começaram a funcionar, muitos já se perguntavam: Eles podem pensar? Em caso
          positivo, um dia eles se equiparariam aos seres humanos? Pensando neste assunto, Turing publicou um artigo em
          1950. <br>uring contribuiu, de maneira muito importante, para a chamada lógica matemática. chamado “Computing Machinery and Intelligence” na revista filosófica Mind, e resumidamente o artigo aborda sobre a capacidade que as máquinas tem de pensar e de serem inteligentes. Ao contrário do que muitos pensam, este artigo não é técnico e específico para profissionais de tecnologia da informação, podendo ser facilmente entendido por profissionais de todas as áreas. Turing inicia o artigo propondo a seguinte questão: “As máquinas podem pensar?”. Porém, como o processo de pensar é de difícil definição, ele trocou a pergunta para: “Há como imaginar um computador digital que faria bem o jogo da imitação?”.
        </p>
      </div>
    </div>
    
    
    <div class="container direita">
      <div class="conteudo">
        <div class="flex">
          <h2>1953</h2>
          <img src="img/1953.jpeg" width="200px">
        </div>
        <p>O nome inteligência artificial ainda não era usado, mas a criação é reconhecida como sua base de funcionamento. Em 1956, John McCarthy utilizou o termo pela primeira vez.</p>
      </div>
    </div>
    
    <div class="container esquerda">
      <div class="conteudo">
        <div class="flex">
          <h2>1958</h2>
          <img src="img/1958.png" width="200px">
        </div>
        <p>O perceptron é um tipo de rede neural artificial inventada em 1958 por Frank Rosenblatt no Cornell Aeronautical Laboratory. Ele pode ser visto como o tipo mais simples de rede neural feedforward: um classificador linear.</p>
      </div>
    </div>

    <div class="container direita">
      <div class="conteudo">
        <div class="flex">
          <h2>1967</h2>
          <img src="img/1967.jpeg" width="200px">
        </div>
        <p> Matemático soviético e ucraniano Alexey Ivakhnenko desenvolveu o método de grupo de tratamento de dados, um método de aprendizagem estatística indutiva, pelo qual às vezes é referido como o "Pai da aprendizagem
          profunda".</p>
        </div>
      </div>
      
      <div class="container esquerda">
        <div class="conteudo">
          <div class="flex">
            <h2>1980</h2>
            <img src="img/1980.jpeg" width="200px">
          </div>
          <p>O NeocognitronOffsite Link, uma rede neural hierárquica multicamada que adquire a capacidade de reconhecer padrões visuais através da aprendizagem, pode ser um dos primeiros exemplos do que mais tarde foi chamado de “aprendizagem Offsite Linkprofunda”. Foi inventado em 1980 por Kunihiko FukushimaOffsite Link, enquanto na NHK Science & Technical Research Laboratories (STRL, NHK, NHK Hs Gijutsu Kenkyjo), com sede em Setagaya, TóquioOffsite Link. O Neocognitron foi utilizado para reconhecimento de caracteres anuscritos Offsite Linke outras tarefas.</p>
        </div>
      </div>

    <div class="container direita">
      <div class="conteudo">
        <div class="flex">
          <h2>1986</h2>
          <img src="img/1986.jpeg" width="200px">
        </div>
        <p>O backpropagation é indiscutivelmente o algoritmo mais importante na história das redes neurais – sem backpropagation, seria quase impossível treinar redes de aprendizagem profunda da forma que vemos hoje. O backpropagation pode ser considerado a pedra angular das redes neurais modernas e consequentemente do Deep Learning. <br>O algoritmo backpropagation foi originalmente introduzido na década de 1970, mas sua importância não foi totalmente apreciada até um famoso artigo de 1986 de David Rumelhart, Geoffrey Hinton e Ronald Williams. Esse artigo descreve várias redes neurais em que o backpropagation funciona muito mais rapidamente do que as abordagens anteriores de aprendizado, possibilitando o uso de redes neurais para resolver problemas que antes eram insolúveis.
        </p>
      </div>
    </div>
    
    <div class="container esquerda">
      <div class="conteudo">
        <div class="flex">
          <h2>1988</h2>
          <img src="img/1988.gif" width="200px">
        </div>
        <p>Yann LeCun et al. propôs a forma original do LeNet que foi uma das primeiras redes neurais convolucionais e promoveu o desenvolvimento do aprendizado profundo . Desde 1988, após anos de pesquisa e muitas iterações bem-sucedidas, o trabalho pioneiro foi denominado LeNet-5 em 1995 tendo aplicações prátivas dez anos depois.</p>
      </div>
    </div>

    <div class="container direita">
      <div class="conteudo">
        <div class="flex">
          <h2>2006</h2>
          <img src="img/2006.jpeg" width="300px">
        </div>
        <p>Criado pelo cientista de dados Fei-Fei Li em 2006, o banco de dados ImageNet contém agora mais de 14 milhões de imagens anotadas. Ele desempenhou um papel fundamental no avanço da visão computacional em aplicações como reconhecimento de objetos, classificação de imagens e localização de objetos. <br>Tudo começou em 1985. George A. Miller e sua equipe da Universidade de Princeton começaram a trabalhar no WordNet, um banco de dados lexical para a língua inglesa. Como um cruzamento entre um dicionário e um tesauro, possibilitou aplicações em Processamento de Linguagem Natural (PNL). <br>Avançando 21 anos, o cientista de dados Fei-Fei Li teve a ideia do ImageNet na Universidade de Illinois Urbana-Champaign. Na época, a maioria dos pesquisadores de IA pensava que os algoritmos eram mais importantes do que os próprios dados. No entanto, Li estava convencido de que grandes quantidades de dados do mundo real tornariam os algoritmos mais precisos. A essa altura, o WordNet estava maduro e a versão 3.0 havia sido lançada recentemente. Quando Li conheceu a pesquisadora do WordNet, Christiane Fellbaum, da Universidade de Princeton, ela decidiu usar a base de palavras e a hierarquia do WordNet para seu ambicioso banco de dados de imagens. Seu objetivo? Apoiar a pesquisa de software de reconhecimento visual de objetos.</p>
      </div>
    </div>

    <div class="container esquerda">
      <div class="conteudo">
        <div class="flex">
          <h2>2009</h2>
          <img src="img/2009.png" width="200px">
        </div>
        <p>Em 2009, descobriu-se que substituir o pré-treinamento por grandes dados em determinados contextos poderia fazer com que as redes neurais profundas superassem em grande parte seus rivais, o modelo de mistura gaussiana (GMM)/modelo oculto de Markov (HMM). <br>A Nvidia criou o que é chamado de 'big bang' no aprendizado profundo, unindo-se a pesquisadores – especialmente do Google – para construir sistemas de aprendizado profundo baseados em GPU que poderiam acelerar os cálculos em 100 vezes. Compiladores de aprendizagem profunda e máquinas virtuais são ativamente desenvolvidos pelos fabricantes de placas gráficas, paralelamente aos processadores tensores, dedicados à “IA”.</p>
      </div>
    </div>

    <div class="container direita">
      <div class="conteudo">
        <div class="flex">
          <h2>2011</h2>
          <img src="img/2011.png" width="200px">
        </div>
        <p>AlexNet é o nome de uma arquitetura de rede neural convolucional (CNN), projetada por Alex Krizhevsky emcolaboração com Ilya Sutskever e Geoffrey Hinton , que foi Ph.D. conselheiro da Universidade de Toronto. <br>AlexNet competiu no ImageNet Large Scale Visual Recognition Challenge em 30 de setembro de 2012. A rede alcançou um erro entre os 5 primeiros de 15,3%, mais de 10,8 pontos percentuais abaixo do segundo colocado. O principal resultado do artigo original foi que a profundidade do modelo era essencial para seu alto desempenho, que era computacionalmente caro, mas viabilizado devido à utilização de unidades de processamento gráfico (GPUs) durante o treinamento.</p>
      </div>
    </div>

    <div class="container esquerda">
      <div class="conteudo">
        <div class="flex">
          <h2>2012</h2>
          <img src="img/2012.png" width="200px">
        </div>
        <p>Os cientistas da computação do misterioso laboratório X do Google construíram uma rede neural de 16 mil processadores de computador com um bilhão de conexões e permitiram que ela navegasse no YouTube, eles fizeram o que muitos usuários da web poderiam fazer: começaram a procurar gatos.</p>
      </div>
    </div>

    <div class="container direita">
      <div class="conteudo">
        <div class="flex">
          <h2>2014</h2>
          <img src="img/2014.jpeg" width="200px">
        </div>
        <p>DeepFace foi produzido por um grupo de cientistas da equipe de pesquisa de inteligência artificial do Facebook . A equipe inclui Yainiv Taigman e um cientista pesquisador do Facebook, Ming Yang. Eles também se juntaram a Lior Wolf, membro do corpo docente da Universidade de Tel Aviv . Yaniv Taigman veio para o Facebook quando o Facebook adquiriu o Face.com em 2012. <br>O usuários no início de 2015 e tem expandido continuamente o uso e o software do DeepFace. O DeepFace, segundo o diretor de pesquisa de inteligência artificial do Facebook , não tem como objetivo invadir a privacidade individual. Em vez disso, o DeepFace alerta os indivíduos quando seu rosto aparece em qualquer foto postada no Facebook. Ao receber esta notificação, eles têm a opção de retirar o rosto da foto.</p>
      </div>
    </div>

    <div class="container esquerda">
      <div class="conteudo">
        <div class="flex">
          <h2>2015</h2>
          <img src="img/2015.jpeg" width="200px">
        </div>
        <p>O Google desenvolveu uma nova inteligência artificial de conversa que usa redes neurais para aprender diálogos de filmes — e ela pode conversar normalmente sobre ética e problemas de VPN. <br>A nova inteligência artificial evita técnicas comuns usadas na criação de chatbots — que geralmente consistem em engenheiros programando regras de como o robô deve lidar com conversas e questões — e usa redes neurais que podem aprender sozinhas.</p>
      </div>
    </div>

    <div class="container direita">
      <div class="conteudo">
        <div class="flex">
          <h2>2016</h2>
          <img src="img/2016.jpeg" width="200px">
        </div>
        <p>AlphaGo versus Lee Sedol, também chamado de Google DeepMind Challenge Match, foi uma disputa de 5 jogos entre um supercomputador da Google, intitulado AlphaGo, contra o sul-coreano Lee Sedol no milenar jogo de tabuleiro Go, entre os dias 9 e 15 Março de 2016 no Four Seasons Hotel da cidade sul-coreana de Seul.</p>
      </div>
    </div>

    <div class="container esquerda">
      <div class="conteudo">
        <div class="flex">
          <h2>2017</h2>
          <img src="img/2017.jpeg" width="200px">
        </div>
        <h3>AlphaGo Zero</h3>
        <p>AlphaGo Zero é uma versão do software Go AlphaGo da DeepMind. A equipe do AlphaGo publicou um artigo na revista Nature em 19 de outubro de 2017, apresentando o AlphaGo Zero, uma versão criada sem usar dados de jogos humanos e mais forte que qualquer versão anterior. Ao jogar contra si mesmo, AlphaGo Zero superou a força de AlphaGo Lee em três dias ao vencer 100 jogos a 0, atingiu o nível de AlphaGo Master em 21 dias e superou todas as versões antigas em 40 dias.</p>
      </div>
    </div>

    <div class="container direita">
      <div class="conteudo">
        <div class="flex">
          <h2>2018</h2>
          <img src="img/2018.jpeg" width="200px">
        </div>
        <h3>Rede Adversarial Generativa (GAN)</h3>
        <p>A GAN, é um tipo de arquitetura de rede neural para modelagem generativa. <br>A modelagem generativa envolve o uso de um modelo para gerar novos exemplos que provêm plausivelmente de uma distribuição existente de amostras, como a geração de novas fotografias que são semelhantes, mas especificamente diferentes de um conjunto de dados de fotografias existentes. <br>Um GAN é um modelo generativo treinado usando dois modelos de rede neural. Um modelo é chamado de modelo “ gerador ” ou “ rede generativa ” que aprende a gerar novas amostras plausíveis. O outro modelo é chamado de “ discriminador ” ou “ rede discriminativa ” e aprende a diferenciar exemplos gerados de exemplos reais.
        <ul>
          <li>Gerar exemplos para conjuntos de dados de imagens</li>
          <li>Gerar fotografias de rostos humanos</li>
          <li>Gere fotografias realistas</li>
          <li>Gerar personagens de desenhos animados</li>
          <li>Tradução imagem para imagem</li>
          <li>Tradução de texto para imagem
          <li>Tradução semântica de imagem para foto</li>
          <li>Geração de visão frontal facial</li>
          <li>Gere novas poses humanas</li>
          <li>Fotos para Emojis</li>
          <li>Edição de fotografia</li>
          <li>Envelhecimento facial
          <li>Mistura de fotos
          <li>Super resolução</li>
          <li>Pintura fotográfica</li>
          <li>Tradução de roupas</li>
          <li>Previsão de vídeo</li>
          <li>Geração de objetos 3D</li>
        </ul>
        </p>
      </div>
    </div>

    <div class="container esquerda">
      <div class="conteudo">
        <div class="flex">
          <h2>2019</h2>
          <img src="img/2019.jpeg" width="200px">
        </div>
        <h3>Deepfake Sansung</h3>
        <p>Pesquisadores da Samsung Labs revelaram uma nova tecnologia de inteligência artificial que promete elevar o nível de criação de deepfakes. O método é capaz de gerar “imagens realistas de alta definição” de diferentes personalidades, utilizando uma única foto de origem.</p>
      </div>
    </div>

    <div class="container direita">
      <div class="conteudo">
        <div class="flex">
          <h2>2020</h2>
          <img src="img/2020.jpeg" width="200px">
        </div>
        <h3>GPT-3</h3>
        <p>Generative Pre-Training Transformer 3 é um modelo de linguagem autorregressivo que usa aprendizagem profunda para produzir texto semelhante ao humano.
        <p>Data de lançamento: 11 de junho de 2020</p>
        <p></p>Desenvolvedor: OpenAI</p>
        <p></p>Estado do desenvolvimento: Beta</p>
        <p></p>Lançamento: 11 de junho de 2020 (3 anos)</p>
        <p></p>Licença: Proprietária</p>
        <p></p>Página oficial: openai.com/blog/openai-api</p>
      </div>
    </div>

    <div class="container esquerda">
      <div class="conteudo">
        <div class="flex">
          <h2>2021</h2>
          <img src="img/2021.jpeg" width="200px">
        </div>
        <h3>Avanços</h3>
        <p>Segundo uma pesquisa feita pelo Capterra, que investigava o uso de chatbots na área da saúde, cerca de 39% dos pacientes tiveram a experiência de interagir com um chatbot ou assistente virtual e, desse total, 61% tiveram essa interação após o início da pandemia. <br>Em 2021, a pandemia da Covid-19 ainda não tinha acabado, em resultado disso, as formas de Inteligência Artificial que possibilitavam a manutenção do isolamento social cresceram durante o período. <br>Esse é o caso dos chatbots, que proporcionaram atendimento aos clientes e resolveram problemas simples, diminuindo a necessidade de buscar assistência presencial. Também foi possível integrar a tecnologia com diferentes plataformas, inclusive redes sociais.</p>
      </div>
    </div>

    <div class="container direita">
      <div class="conteudo">
        <div class="flex">
          <h2>2022</h2>
          <img src="img/2022.jpeg" width="200px">
        </div>
        <h3>Mais Avanços</h3>
        <p>
        <ul>
          <li>Expansão da criatividade</li>
          <li>Aumento de comandos por voz</li>
          <li>Melhor compreensão da linguagem</li>
        </ul>
        </p>
      </div>
    </div>

    <div class="container esquerda">
      <div class="conteudo">
        <div class="flex">
          <h2>2023</h2>
          <img src="img/2023.jpeg" width="200px">
        </div>
        <h3>Destaques</h3>
        <p>
        <ul>
          <li>O ano dos chatbots</li>
          <li>IAs para geração de imagens</li>
          <li>IA em tudo</li>
          <li>Impacto sobre empregos</li>
          <li>Regulamentação e direitos autorais</li>
        </ul>
        </p>
      </div>
    </div>

    <div class="container direita">
      <div class="conteudo">
        <div class="flex">
          <h2>2024</h2>
          <img src="img/2024.jpeg" width="200px">
        </div>
        <h3>Novidades - Como a IA fará parte do seu dia a dia em 2024</h3>
        <p>A CNN ouviu especialistas em tecnologia e inteligência artificial para entender como a IA deve estar cada vez mais presente em nossa rotina a partir deste ano e o que deve mudar na prática com essas novas possibilidades e ferramentas. <br>“Acho que a gente vai começar a incorporar cada vez mais esse tipo de ferramenta no nosso dia a dia, seja para nos auxiliar no processo de produção de algum conteúdo ou com alguma coisa que a gente precisa fazer, como também auxiliar a gente em processos de pesquisa, de busca de informação”, afirma Diogo Cortiz, professor da PUC-SP. <br>Segundo Cortiz, a IA deve passar a funcionar “mais ou menos como um companheiro de trabalho para nos auxiliar em diversas tarefas”. <br>No fim das contas, a grande mudança que a inteligência artificial traz é na forma como se dá o diálogo com a tecnologia. <br>Se desde a criação dos softwares estamos acostumamos a dar comandos para a tecnologia executar, agora será uma questão de pedir. E é bem mais fácil fazer um pedido do que memorizar cada comando e o que ele faz. </p>
      </div>
    </div>

  </div>
  


</body>

</html>